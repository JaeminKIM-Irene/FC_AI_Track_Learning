{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaeminKIM-Irene/FC_AI_Track_Learning/blob/main/CV/tutorial/%5BCV_realtime%5D5.%20torchvision_finetuning_instance_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O57vQCApNvoU"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_2jKhdyNvoW"
      },
      "source": [
        "\n",
        "# TorchVision Object Detection Finetuning Tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SwTjK2_NvoW"
      },
      "source": [
        "\n",
        "본 튜토리얼에서는 Penn-Fudan Database for Pedestrian Detection and Segmentation 데이터셋으로 미리 학습된 Mask R-CNN 모델을 미세조정 해 볼 것입니다. 이 데이터셋에는 보행자 인스턴스(instance, 역자주: 이미지 내에서 사람의 위치 좌표와 픽셀 단위의 사람 여부를 구분한 정보를 포함합니다.) 345명이 있는 170개의 이미지가 포함되어 있으며, 우리는 이 이미지를 사용하여 사용자 정의 데이터셋에 인스턴스 분할(Instance Segmentation) 모델을 학습하기 위해 torchvision의 새로운 기능을 사용하는 방법을 설명 할 예정입니다.\n",
        "\n",
        "\n",
        "\n",
        "## Defining the Dataset\n",
        "\n",
        "객체 검출, 인스턴스 분할 및 사용자 키포인트(Keypoint) 검출을 학습하기 위한 참조 스크립트를 통해 새로운 사용자 정의 데이터셋 추가를 쉽게 진행해 볼 수 있습니다. 데이터셋은 표준 torch.utils.data.Dataset 클래스를 상속 받아야 하며, __len__ 와 __getitem__ 메소드를 구현해 주어야 합니다.\n",
        "\n",
        "데이터셋에서 필요한 유일한 특성은 __getitem__ 메소드가 다음을 반환 해야 하는 것입니다:\n",
        "\n",
        "> ```이미지``` : PIL(Python Image Library) 이미지의 크기 (H, W)\n",
        "\n",
        "> ```대상```: 다음의 필드를 포함하는 사전 타입\n",
        "\n",
        "> ```boxes (FloatTensor[N, 4])``` : N 개의 바운딩 박스(Bounding box)의 좌표를 [x0, y0, x1, y1] 형태로 가집니다. x와 관련된 값 범위는 0 부터 W 이고 y와 관련된 값의 범위는 0 부터 H 까지입니다.\n",
        "\n",
        "> ```labels (Int64Tensor[N])``` : 바운딩 박스 마다의 라벨 정보입니다. 0 은 항상 배경의 클래스를 표현합니다.\n",
        "\n",
        "> ```image_id (Int64Tensor[1])```: 이미지 구분자입니다. 데이터셋의 모든 이미지 간에 고유한 값이어야 하며 평가 중에도 사용됩니다.\n",
        "\n",
        "> ```area (Tensor[N])```: 바운딩 박스의 면적입니다. 면적은 평가 시 작음,중간,큰 박스 간의 점수를 내기 위한 기준이며 COCO 평가를 기준으로 합니다.\n",
        "\n",
        "> ```iscrowd (UInt8Tensor[N])```: 이 값이 참일 경우 평가에서 제외합니다.\n",
        "\n",
        "> ```(선택적) masks (UInt8Tensor[N, H, W])```: N 개의 객체 마다의 분할 마스크 정보입니다.\n",
        "\n",
        "> ```(선택적) keypoints (FloatTensor[N, K, 3])```: N 개의 객체마다의 키포인트 정보입니다. 키포인트는 [x, y, visibility] 형태의 값입니다. visibility 값이 0인 경우 키포인트는 보이지 않음을 의미합니다. 데이터 증강(Data augmentation)의 경우 키포인트 좌우 반전의 개념은 데이터 표현에 따라 달라지며, 새로운 키포인트 표현에 대해 《references/detection/transforms.py》 코드 부분을 수정 해야 할 수도 있습니다.\n",
        "\n",
        "모델이 위의 방법대로 리턴을 하면, 학습과 평가 둘 다에 대해서 동작을 할 것이며 평가 스크립트는 pip install pycocotools` 로 설치 가능한 ```pycocotools``` 를 사용하게 될 것입니다.\n",
        "\n",
        "\n",
        "\n",
        "### labels 에 대한 참고사항.\n",
        "\n",
        "이 모델은 클래스 0 을 배경으로 취급합니다. 만약 준비한 데이터셋에 배경의 클래스가 없다면, labels 에도 0 이 없어야 합니다. 예를 들어, 고양이 와 강아지 의 오직 2개의 클래스만 분류한다고 가정하면, (0 이 아닌) 1 이 고양이 를, 2 가 강아지 를 나타내도록 정의해야 합니다. 따라서, 이 예시에서, 어떤 이미지에 두 개의 클래스를 모두 있다면, labels 텐서는 [1,2] 와 같은 식이 되어야 합니다.\n",
        "\n",
        "추가로, 학습 중에 가로 세로 비율 그룹화를 사용하려는 경우(각 배치에 유사한 가로 세로 비율이 있는 영상만 포함되도록), 이미지의 넓이, 높이를 리턴할 수 있도록 get_height_and_width 메소드를 구현하기를 추천합니다. 이 메소드가 구현되지 않은 경우에는 모든 데이터셋은 __getitem__ 를 통해 메모리에 이미지가 로드되며 사용자 정의 메소드를 제공하는 것보다 느릴 수 있습니다.\n",
        "\n",
        "### Writing a custom dataset for PennFudan\n",
        "\n",
        "PennFudan 데이터셋을 위한 코드를 작성해 보겠습니다. `다운로드 후 압축 파일을 해제하면<https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip>`__, 다음의 폴더 구조를 볼 수 있습니다:\n",
        "::\n",
        "```\n",
        "   PennFudanPed/\n",
        "     PedMasks/\n",
        "       FudanPed00001_mask.png\n",
        "       FudanPed00002_mask.png\n",
        "       FudanPed00003_mask.png\n",
        "       FudanPed00004_mask.png\n",
        "       ...\n",
        "     PNGImages/\n",
        "       FudanPed00001.png\n",
        "       FudanPed00002.png\n",
        "       FudanPed00003.png\n",
        "       FudanPed00004.png\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1R0xkrNvoX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "\n",
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # 모든 이미지 파일들을 읽고, 정렬하여\n",
        "        # 이미지와 분할 마스크 정렬을 확인합니다\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 이미지와 마스크를 읽어옵니다\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        # 분할 마스크는 RGB로 변환하지 않음을 유의하세요\n",
        "        # 왜냐하면 각 색상은 다른 인스턴스에 해당하며, 0은 배경에 해당합니다\n",
        "        img = read_image(img_path)\n",
        "        mask = read_image(mask_path)\n",
        "\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = torch.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        # 컬러 인코딩된 마스크를 바이너리 마스크 세트로 나눕니다\n",
        "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "        # 객체 종류는 한 종류만 존재합니다. 예제에서는 사람만이 대상입니다\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        image_id = idx\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # 모든 인스턴스는 군중(crowd) 상태가 아님을 가정합니다\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Wrap sample and targets into torchvision tv_tensors:\n",
        "        img = tv_tensors.Image(img)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AugLANm9NvoX"
      },
      "source": [
        "## Defining your model\n",
        "\n",
        "Torchvision model zoo, (미리 학습된 모델들을 모아 놓은 공간)에서 사용 가능한 모델들 중 하나를 이용해 모델을 수정하려면 보통 두가지 상황이 있습니다. 첫 번째 방법은 미리 학습된 모델에서 시작해서 마지막 레이어 수준만 미세 조정하는 것입니다. 다른 하나는 모델의 백본을 다른 백본으로 교체하는 것입니다.\n",
        "\n",
        "역자주: 백본 모델을 ResNet101 에서 MobilenetV2 로 교체하면 수행 속도 향상을 기대할 수 있습니다. 대신 인식 성능은 저하 될 수 있습니다.\n",
        "\n",
        "\n",
        "### 1 - Finetuning from a pretrained model\n",
        "\n",
        "COCO에 대해 미리 학습된 모델에서 시작하여 특정 클래스를 위해 미세 조정을 원한다고 가정해 봅시다. 아래와 같은 방법으로 가능합니다:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOQVvvCyNvoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c71646-819e-4e0f-b2d9-3e6a8996add8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:02<00:00, 68.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# COCO로 미리 학습된 모델 읽기\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "# 분류기를 새로운 것으로 교체하는데, num_classes는 사용자가 정의합니다\n",
        "num_classes = 2  # 1 클래스(사람) + 배경\n",
        "\n",
        "# 분류기에서 사용할 입력 특징의 차원 정보를 얻습니다\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# 미리 학습된 모델의 머리 부분을 새로운 것으로 교체합니다\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.roi_heads.box_predictor.cls_score.in_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhTT5k6ybUG2",
        "outputId": "d29fdbaa-260a-4656-aeee-2c1d380e3188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i7l4okPNvoY"
      },
      "source": [
        "### 2 - Modifying the model to add a different backbone\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJfqFGtfNvoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4b2773-0e3c-4ba7-eeee-496d63659360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 70.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# 분류 목적으로 미리 학습된 모델을 로드하고 특징들만을 리턴하도록 합니다\n",
        "backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n",
        "# Faster RCNN은 백본의 출력 채널 수를 알아야 합니다.\n",
        "# mobilenetV2의 경우 1280이므로 여기에 추가해야 합니다.\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "# RPN(Region Proposal Network)이 5개의 서로 다른 크기와 3개의 다른 측면 비율(Aspect ratio)을 가진\n",
        "# 5 x 3개의 앵커를 공간 위치마다 생성하도록 합니다.\n",
        "# 각 특징 맵이 잠재적으로 다른 사이즈와 측면 비율을 가질 수 있기 때문에 Tuple[Tuple[int]] 타입을 가지도록 합니다.\n",
        "\n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 1, 2),))\n",
        "\n",
        "# 관심 영역의 자르기 및 재할당 후 자르기 크기를 수행하는 데 사용할 피쳐 맵을 정의합니다.\n",
        "# 만약 백본이 텐서를 리턴할때, featmap_names 는 [0] 이 될 것이라고 예상합니다.\n",
        "# 일반적으로 백본은 OrderedDict[Tensor] 타입을 리턴해야 합니다.\n",
        "# 그리고 특징맵에서 사용할 featmap_names 값을 정할 수 있습니다.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "# 조각들을 Faster RCNN 모델로 합칩니다.\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=2,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model structure: {backbone}\\n\\n\")\n",
        "\n",
        "for name, param in backbone.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKhEjNcTaQDI",
        "outputId": "c5a08554-a3eb-4ad4-d849-6edf6768d8dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: Sequential(\n",
            "  (0): Conv2dNormActivation(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU6(inplace=True)\n",
            "  )\n",
            "  (1): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (2): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
            "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (3): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
            "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (4): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
            "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (5): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (6): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (7): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
            "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (8): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (9): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (10): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (11): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (12): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
            "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (13): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
            "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (14): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
            "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (15): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
            "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (16): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
            "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (17): InvertedResidual(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2dNormActivation(\n",
            "        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
            "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (18): Conv2dNormActivation(\n",
            "    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU6(inplace=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: 0.0.weight | Size: torch.Size([32, 3, 3, 3]) \n",
            "\n",
            "Layer: 0.1.weight | Size: torch.Size([32]) \n",
            "\n",
            "Layer: 0.1.bias | Size: torch.Size([32]) \n",
            "\n",
            "Layer: 1.conv.0.0.weight | Size: torch.Size([32, 1, 3, 3]) \n",
            "\n",
            "Layer: 1.conv.0.1.weight | Size: torch.Size([32]) \n",
            "\n",
            "Layer: 1.conv.0.1.bias | Size: torch.Size([32]) \n",
            "\n",
            "Layer: 1.conv.1.weight | Size: torch.Size([16, 32, 1, 1]) \n",
            "\n",
            "Layer: 1.conv.2.weight | Size: torch.Size([16]) \n",
            "\n",
            "Layer: 1.conv.2.bias | Size: torch.Size([16]) \n",
            "\n",
            "Layer: 2.conv.0.0.weight | Size: torch.Size([96, 16, 1, 1]) \n",
            "\n",
            "Layer: 2.conv.0.1.weight | Size: torch.Size([96]) \n",
            "\n",
            "Layer: 2.conv.0.1.bias | Size: torch.Size([96]) \n",
            "\n",
            "Layer: 2.conv.1.0.weight | Size: torch.Size([96, 1, 3, 3]) \n",
            "\n",
            "Layer: 2.conv.1.1.weight | Size: torch.Size([96]) \n",
            "\n",
            "Layer: 2.conv.1.1.bias | Size: torch.Size([96]) \n",
            "\n",
            "Layer: 2.conv.2.weight | Size: torch.Size([24, 96, 1, 1]) \n",
            "\n",
            "Layer: 2.conv.3.weight | Size: torch.Size([24]) \n",
            "\n",
            "Layer: 2.conv.3.bias | Size: torch.Size([24]) \n",
            "\n",
            "Layer: 3.conv.0.0.weight | Size: torch.Size([144, 24, 1, 1]) \n",
            "\n",
            "Layer: 3.conv.0.1.weight | Size: torch.Size([144]) \n",
            "\n",
            "Layer: 3.conv.0.1.bias | Size: torch.Size([144]) \n",
            "\n",
            "Layer: 3.conv.1.0.weight | Size: torch.Size([144, 1, 3, 3]) \n",
            "\n",
            "Layer: 3.conv.1.1.weight | Size: torch.Size([144]) \n",
            "\n",
            "Layer: 3.conv.1.1.bias | Size: torch.Size([144]) \n",
            "\n",
            "Layer: 3.conv.2.weight | Size: torch.Size([24, 144, 1, 1]) \n",
            "\n",
            "Layer: 3.conv.3.weight | Size: torch.Size([24]) \n",
            "\n",
            "Layer: 3.conv.3.bias | Size: torch.Size([24]) \n",
            "\n",
            "Layer: 4.conv.0.0.weight | Size: torch.Size([144, 24, 1, 1]) \n",
            "\n",
            "Layer: 4.conv.0.1.weight | Size: torch.Size([144]) \n",
            "\n",
            "Layer: 4.conv.0.1.bias | Size: torch.Size([144]) \n",
            "\n",
            "Layer: 4.conv.1.0.weight | Size: torch.Size([144, 1, 3, 3]) \n",
            "\n",
            "Layer: 4.conv.1.1.weight | Size: torch.Size([144]) \n",
            "\n",
            "Layer: 4.conv.1.1.bias | Size: torch.Size([144]) \n",
            "\n",
            "Layer: 4.conv.2.weight | Size: torch.Size([32, 144, 1, 1]) \n",
            "\n",
            "Layer: 4.conv.3.weight | Size: torch.Size([32]) \n",
            "\n",
            "Layer: 4.conv.3.bias | Size: torch.Size([32]) \n",
            "\n",
            "Layer: 5.conv.0.0.weight | Size: torch.Size([192, 32, 1, 1]) \n",
            "\n",
            "Layer: 5.conv.0.1.weight | Size: torch.Size([192]) \n",
            "\n",
            "Layer: 5.conv.0.1.bias | Size: torch.Size([192]) \n",
            "\n",
            "Layer: 5.conv.1.0.weight | Size: torch.Size([192, 1, 3, 3]) \n",
            "\n",
            "Layer: 5.conv.1.1.weight | Size: torch.Size([192]) \n",
            "\n",
            "Layer: 5.conv.1.1.bias | Size: torch.Size([192]) \n",
            "\n",
            "Layer: 5.conv.2.weight | Size: torch.Size([32, 192, 1, 1]) \n",
            "\n",
            "Layer: 5.conv.3.weight | Size: torch.Size([32]) \n",
            "\n",
            "Layer: 5.conv.3.bias | Size: torch.Size([32]) \n",
            "\n",
            "Layer: 6.conv.0.0.weight | Size: torch.Size([192, 32, 1, 1]) \n",
            "\n",
            "Layer: 6.conv.0.1.weight | Size: torch.Size([192]) \n",
            "\n",
            "Layer: 6.conv.0.1.bias | Size: torch.Size([192]) \n",
            "\n",
            "Layer: 6.conv.1.0.weight | Size: torch.Size([192, 1, 3, 3]) \n",
            "\n",
            "Layer: 6.conv.1.1.weight | Size: torch.Size([192]) \n",
            "\n",
            "Layer: 6.conv.1.1.bias | Size: torch.Size([192]) \n",
            "\n",
            "Layer: 6.conv.2.weight | Size: torch.Size([32, 192, 1, 1]) \n",
            "\n",
            "Layer: 6.conv.3.weight | Size: torch.Size([32]) \n",
            "\n",
            "Layer: 6.conv.3.bias | Size: torch.Size([32]) \n",
            "\n",
            "Layer: 7.conv.0.0.weight | Size: torch.Size([192, 32, 1, 1]) \n",
            "\n",
            "Layer: 7.conv.0.1.weight | Size: torch.Size([192]) \n",
            "\n",
            "Layer: 7.conv.0.1.bias | Size: torch.Size([192]) \n",
            "\n",
            "Layer: 7.conv.1.0.weight | Size: torch.Size([192, 1, 3, 3]) \n",
            "\n",
            "Layer: 7.conv.1.1.weight | Size: torch.Size([192]) \n",
            "\n",
            "Layer: 7.conv.1.1.bias | Size: torch.Size([192]) \n",
            "\n",
            "Layer: 7.conv.2.weight | Size: torch.Size([64, 192, 1, 1]) \n",
            "\n",
            "Layer: 7.conv.3.weight | Size: torch.Size([64]) \n",
            "\n",
            "Layer: 7.conv.3.bias | Size: torch.Size([64]) \n",
            "\n",
            "Layer: 8.conv.0.0.weight | Size: torch.Size([384, 64, 1, 1]) \n",
            "\n",
            "Layer: 8.conv.0.1.weight | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 8.conv.0.1.bias | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 8.conv.1.0.weight | Size: torch.Size([384, 1, 3, 3]) \n",
            "\n",
            "Layer: 8.conv.1.1.weight | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 8.conv.1.1.bias | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 8.conv.2.weight | Size: torch.Size([64, 384, 1, 1]) \n",
            "\n",
            "Layer: 8.conv.3.weight | Size: torch.Size([64]) \n",
            "\n",
            "Layer: 8.conv.3.bias | Size: torch.Size([64]) \n",
            "\n",
            "Layer: 9.conv.0.0.weight | Size: torch.Size([384, 64, 1, 1]) \n",
            "\n",
            "Layer: 9.conv.0.1.weight | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 9.conv.0.1.bias | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 9.conv.1.0.weight | Size: torch.Size([384, 1, 3, 3]) \n",
            "\n",
            "Layer: 9.conv.1.1.weight | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 9.conv.1.1.bias | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 9.conv.2.weight | Size: torch.Size([64, 384, 1, 1]) \n",
            "\n",
            "Layer: 9.conv.3.weight | Size: torch.Size([64]) \n",
            "\n",
            "Layer: 9.conv.3.bias | Size: torch.Size([64]) \n",
            "\n",
            "Layer: 10.conv.0.0.weight | Size: torch.Size([384, 64, 1, 1]) \n",
            "\n",
            "Layer: 10.conv.0.1.weight | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 10.conv.0.1.bias | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 10.conv.1.0.weight | Size: torch.Size([384, 1, 3, 3]) \n",
            "\n",
            "Layer: 10.conv.1.1.weight | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 10.conv.1.1.bias | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 10.conv.2.weight | Size: torch.Size([64, 384, 1, 1]) \n",
            "\n",
            "Layer: 10.conv.3.weight | Size: torch.Size([64]) \n",
            "\n",
            "Layer: 10.conv.3.bias | Size: torch.Size([64]) \n",
            "\n",
            "Layer: 11.conv.0.0.weight | Size: torch.Size([384, 64, 1, 1]) \n",
            "\n",
            "Layer: 11.conv.0.1.weight | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 11.conv.0.1.bias | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 11.conv.1.0.weight | Size: torch.Size([384, 1, 3, 3]) \n",
            "\n",
            "Layer: 11.conv.1.1.weight | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 11.conv.1.1.bias | Size: torch.Size([384]) \n",
            "\n",
            "Layer: 11.conv.2.weight | Size: torch.Size([96, 384, 1, 1]) \n",
            "\n",
            "Layer: 11.conv.3.weight | Size: torch.Size([96]) \n",
            "\n",
            "Layer: 11.conv.3.bias | Size: torch.Size([96]) \n",
            "\n",
            "Layer: 12.conv.0.0.weight | Size: torch.Size([576, 96, 1, 1]) \n",
            "\n",
            "Layer: 12.conv.0.1.weight | Size: torch.Size([576]) \n",
            "\n",
            "Layer: 12.conv.0.1.bias | Size: torch.Size([576]) \n",
            "\n",
            "Layer: 12.conv.1.0.weight | Size: torch.Size([576, 1, 3, 3]) \n",
            "\n",
            "Layer: 12.conv.1.1.weight | Size: torch.Size([576]) \n",
            "\n",
            "Layer: 12.conv.1.1.bias | Size: torch.Size([576]) \n",
            "\n",
            "Layer: 12.conv.2.weight | Size: torch.Size([96, 576, 1, 1]) \n",
            "\n",
            "Layer: 12.conv.3.weight | Size: torch.Size([96]) \n",
            "\n",
            "Layer: 12.conv.3.bias | Size: torch.Size([96]) \n",
            "\n",
            "Layer: 13.conv.0.0.weight | Size: torch.Size([576, 96, 1, 1]) \n",
            "\n",
            "Layer: 13.conv.0.1.weight | Size: torch.Size([576]) \n",
            "\n",
            "Layer: 13.conv.0.1.bias | Size: torch.Size([576]) \n",
            "\n",
            "Layer: 13.conv.1.0.weight | Size: torch.Size([576, 1, 3, 3]) \n",
            "\n",
            "Layer: 13.conv.1.1.weight | Size: torch.Size([576]) \n",
            "\n",
            "Layer: 13.conv.1.1.bias | Size: torch.Size([576]) \n",
            "\n",
            "Layer: 13.conv.2.weight | Size: torch.Size([96, 576, 1, 1]) \n",
            "\n",
            "Layer: 13.conv.3.weight | Size: torch.Size([96]) \n",
            "\n",
            "Layer: 13.conv.3.bias | Size: torch.Size([96]) \n",
            "\n",
            "Layer: 14.conv.0.0.weight | Size: torch.Size([576, 96, 1, 1]) \n",
            "\n",
            "Layer: 14.conv.0.1.weight | Size: torch.Size([576]) \n",
            "\n",
            "Layer: 14.conv.0.1.bias | Size: torch.Size([576]) \n",
            "\n",
            "Layer: 14.conv.1.0.weight | Size: torch.Size([576, 1, 3, 3]) \n",
            "\n",
            "Layer: 14.conv.1.1.weight | Size: torch.Size([576]) \n",
            "\n",
            "Layer: 14.conv.1.1.bias | Size: torch.Size([576]) \n",
            "\n",
            "Layer: 14.conv.2.weight | Size: torch.Size([160, 576, 1, 1]) \n",
            "\n",
            "Layer: 14.conv.3.weight | Size: torch.Size([160]) \n",
            "\n",
            "Layer: 14.conv.3.bias | Size: torch.Size([160]) \n",
            "\n",
            "Layer: 15.conv.0.0.weight | Size: torch.Size([960, 160, 1, 1]) \n",
            "\n",
            "Layer: 15.conv.0.1.weight | Size: torch.Size([960]) \n",
            "\n",
            "Layer: 15.conv.0.1.bias | Size: torch.Size([960]) \n",
            "\n",
            "Layer: 15.conv.1.0.weight | Size: torch.Size([960, 1, 3, 3]) \n",
            "\n",
            "Layer: 15.conv.1.1.weight | Size: torch.Size([960]) \n",
            "\n",
            "Layer: 15.conv.1.1.bias | Size: torch.Size([960]) \n",
            "\n",
            "Layer: 15.conv.2.weight | Size: torch.Size([160, 960, 1, 1]) \n",
            "\n",
            "Layer: 15.conv.3.weight | Size: torch.Size([160]) \n",
            "\n",
            "Layer: 15.conv.3.bias | Size: torch.Size([160]) \n",
            "\n",
            "Layer: 16.conv.0.0.weight | Size: torch.Size([960, 160, 1, 1]) \n",
            "\n",
            "Layer: 16.conv.0.1.weight | Size: torch.Size([960]) \n",
            "\n",
            "Layer: 16.conv.0.1.bias | Size: torch.Size([960]) \n",
            "\n",
            "Layer: 16.conv.1.0.weight | Size: torch.Size([960, 1, 3, 3]) \n",
            "\n",
            "Layer: 16.conv.1.1.weight | Size: torch.Size([960]) \n",
            "\n",
            "Layer: 16.conv.1.1.bias | Size: torch.Size([960]) \n",
            "\n",
            "Layer: 16.conv.2.weight | Size: torch.Size([160, 960, 1, 1]) \n",
            "\n",
            "Layer: 16.conv.3.weight | Size: torch.Size([160]) \n",
            "\n",
            "Layer: 16.conv.3.bias | Size: torch.Size([160]) \n",
            "\n",
            "Layer: 17.conv.0.0.weight | Size: torch.Size([960, 160, 1, 1]) \n",
            "\n",
            "Layer: 17.conv.0.1.weight | Size: torch.Size([960]) \n",
            "\n",
            "Layer: 17.conv.0.1.bias | Size: torch.Size([960]) \n",
            "\n",
            "Layer: 17.conv.1.0.weight | Size: torch.Size([960, 1, 3, 3]) \n",
            "\n",
            "Layer: 17.conv.1.1.weight | Size: torch.Size([960]) \n",
            "\n",
            "Layer: 17.conv.1.1.bias | Size: torch.Size([960]) \n",
            "\n",
            "Layer: 17.conv.2.weight | Size: torch.Size([320, 960, 1, 1]) \n",
            "\n",
            "Layer: 17.conv.3.weight | Size: torch.Size([320]) \n",
            "\n",
            "Layer: 17.conv.3.bias | Size: torch.Size([320]) \n",
            "\n",
            "Layer: 18.0.weight | Size: torch.Size([1280, 320, 1, 1]) \n",
            "\n",
            "Layer: 18.1.weight | Size: torch.Size([1280]) \n",
            "\n",
            "Layer: 18.1.bias | Size: torch.Size([1280]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2CwCj3NvoY"
      },
      "source": [
        "### Object detection and instance segmentation model for PennFudan Dataset\n",
        "\n",
        "우리의 경우, 데이터 세트가 매우 작기 때문에, 우리는 1번 접근법을 따를 것이라는 점을 고려하여 미리 학습된 모델에서 미세 조정하는 방식으로 진행 하겠습니다.\n",
        "\n",
        "여기서 인스턴스 분할 마스크도 계산하기를 원하기 때문에 Mask R-CNN를 사용합니다:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ0AWM8UNvoZ"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "# 여기는 Segmentation을 위한 함수\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    # COCO 에서 미리 학습된 인스턴스 분할 모델을 읽어옵니다.\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    # 분류를 위한 입력 특징 차원을 얻습니다.\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # 미리 학습된 헤더를 새로운 것으로 바꿉니다\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # 마스크 분류기를 위한 입력 특징들의 차원을 얻습니다\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # 마스크 예측기를 새로운 것으로 바꿉니다\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "        in_features_mask,\n",
        "        hidden_layer,\n",
        "        num_classes\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RgdNc4sNvoZ"
      },
      "source": [
        "그렇습니다. 이렇게 하면 모델 을 사용자 정의 데이터셋에서 학습하고 평가할 준비가 될 겁니다.\n",
        "\n",
        "## Putting everything together\n",
        "\n",
        "``references/detection/`` 폴더 내에 검출 모델들의 학습과 평과를 쉽게 하기 위한 도움 함수들이 있습니다. 여기서 ``references/detection/engine.py``, ``references/detection/utils.py``, ``references/detection/transforms.py`` 를 사용 할 것입니다. ``references/detection`` 아래의 모든 파일과 폴더들을 사용자의 폴더로 복사한 뒤 사용합니다.\n",
        "\n",
        "데이터 증강 / 변환을 위한 도움 함수를 작성해 봅시다\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "os.system(\"wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\")\n",
        "\n",
        "def unzip_file(zip_path, extract_to):\n",
        "    \"\"\"\n",
        "    zip_path: 압축 해제할 ZIP 파일의 경로\n",
        "    extract_to: 압축을 풀 폴더의 경로\n",
        "    \"\"\"\n",
        "    # 압축을 풀 폴더가 없으면 생성\n",
        "    if not os.path.exists(extract_to):\n",
        "        os.makedirs(extract_to)\n",
        "\n",
        "    # ZIP 파일 열기\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # ZIP 파일 내용을 지정된 폴더에 압축 해제\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# 사용 예시\n",
        "zip_file_path = '/content/PennFudanPed.zip'  # ZIP 파일 경로\n",
        "destination_folder = '/content/data/'  # 압축 해제될 폴더 경로\n",
        "\n",
        "unzip_file(zip_file_path, destination_folder)"
      ],
      "metadata": {
        "id": "AMplj-npO5Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u5rrNIcNvoZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c0e6fe-368a-475a-b3f4-4622f4aaa12f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss_classifier': tensor(0.6722, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0497, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.6924, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0638, grad_fn=<DivBackward0>)}\n",
            "{'boxes': tensor([[8.4757e-03, 8.4242e+00, 4.9148e+00, 2.2252e+01],\n",
            "        [0.0000e+00, 1.8917e+01, 8.0957e+00, 3.1016e+01],\n",
            "        [6.3913e+00, 1.7397e+01, 1.9412e+01, 3.0886e+01],\n",
            "        [0.0000e+00, 2.8286e+01, 4.6500e+00, 4.4743e+01],\n",
            "        [6.4574e+00, 2.9429e+01, 1.9176e+01, 4.2293e+01],\n",
            "        [0.0000e+00, 1.8126e+02, 1.0366e+02, 3.0000e+02],\n",
            "        [1.3265e+02, 2.1487e+02, 1.8455e+02, 2.6529e+02],\n",
            "        [0.0000e+00, 0.0000e+00, 3.4116e+01, 2.9297e+01],\n",
            "        [1.2075e+02, 8.3306e+01, 1.7144e+02, 1.3413e+02],\n",
            "        [2.2907e+02, 8.1671e+01, 2.8034e+02, 1.3287e+02],\n",
            "        [7.4338e+01, 1.7909e+02, 1.2436e+02, 2.2892e+02],\n",
            "        [3.6386e+01, 1.6636e+02, 8.7388e+01, 2.1740e+02],\n",
            "        [3.6069e+01, 2.3809e+02, 8.6003e+01, 2.8885e+02],\n",
            "        [1.9347e+02, 1.3100e+02, 2.4457e+02, 1.8159e+02],\n",
            "        [8.3698e+01, 2.2602e+02, 1.3448e+02, 2.7830e+02],\n",
            "        [2.5636e+00, 1.4086e+02, 5.3868e+01, 1.9138e+02],\n",
            "        [1.2153e+02, 2.3846e+02, 1.7192e+02, 2.8943e+02],\n",
            "        [1.4541e+02, 2.2562e+02, 1.9727e+02, 2.7727e+02],\n",
            "        [3.2438e+02, 9.3861e+01, 3.7613e+02, 1.4667e+02],\n",
            "        [1.0947e+02, 2.2755e+02, 1.5948e+02, 2.7838e+02],\n",
            "        [5.9748e+01, 2.6654e+02, 3.2635e+02, 3.0000e+02],\n",
            "        [1.4409e+02, 7.1665e+01, 1.9481e+02, 1.2196e+02],\n",
            "        [2.7341e+00, 2.3554e+02, 5.3817e+01, 2.8773e+02],\n",
            "        [1.4333e+02, 2.0145e+02, 1.9451e+02, 2.5391e+02],\n",
            "        [4.8652e+01, 2.1884e+02, 2.3276e+02, 3.0000e+02],\n",
            "        [7.2857e+01, 2.3867e+02, 1.2334e+02, 2.8969e+02],\n",
            "        [2.0378e+02, 2.6361e+02, 2.5224e+02, 3.0000e+02],\n",
            "        [3.0094e+02, 1.4307e+02, 3.5165e+02, 1.9355e+02],\n",
            "        [1.0720e+02, 9.4263e+01, 1.5841e+02, 1.4569e+02],\n",
            "        [8.1317e+01, 0.0000e+00, 1.3441e+02, 3.4319e+01],\n",
            "        [1.3265e+01, 2.0374e+02, 6.2740e+01, 2.5448e+02],\n",
            "        [3.4229e-01, 1.0243e+01, 5.1959e+01, 6.2121e+01],\n",
            "        [8.2879e+01, 2.6172e+02, 1.3305e+02, 3.0000e+02],\n",
            "        [3.3560e+02, 1.7924e+02, 3.8782e+02, 2.2881e+02],\n",
            "        [2.0520e+02, 8.2620e+01, 2.5645e+02, 1.3314e+02],\n",
            "        [1.6888e+02, 7.0553e+01, 2.1876e+02, 1.2098e+02],\n",
            "        [1.8128e+02, 1.4187e+02, 2.3191e+02, 1.9334e+02],\n",
            "        [2.5018e+01, 1.9124e+02, 7.5453e+01, 2.4177e+02],\n",
            "        [1.6773e+02, 2.0218e+02, 2.1792e+02, 2.5325e+02],\n",
            "        [7.2190e+01, 3.4938e+01, 1.2343e+02, 8.5219e+01],\n",
            "        [4.7740e+01, 2.1487e+02, 9.9360e+01, 2.6504e+02],\n",
            "        [1.3137e+02, 5.9460e+01, 1.8163e+02, 1.0979e+02],\n",
            "        [1.5659e+02, 1.4329e+02, 2.0668e+02, 1.9386e+02],\n",
            "        [3.1263e+02, 1.1896e+02, 3.6446e+02, 1.7025e+02],\n",
            "        [3.9112e+02, 0.0000e+00, 4.0000e+02, 5.8885e+00],\n",
            "        [1.2087e+02, 3.5228e+01, 1.7158e+02, 8.5361e+01],\n",
            "        [1.9221e+02, 9.4420e+01, 2.4241e+02, 1.4423e+02],\n",
            "        [2.4349e+02, 1.8775e+02, 4.0000e+02, 3.0000e+02],\n",
            "        [2.3989e+02, 9.4485e+01, 2.9083e+02, 1.4523e+02],\n",
            "        [3.1162e+02, 2.6341e+02, 3.6073e+02, 3.0000e+02],\n",
            "        [1.1932e+02, 2.0309e+02, 1.7037e+02, 2.5307e+02],\n",
            "        [1.2158e+02, 2.1764e+02, 3.0466e+02, 3.0000e+02],\n",
            "        [2.0535e+02, 1.4324e+02, 2.5528e+02, 1.9405e+02],\n",
            "        [1.3292e+02, 1.4317e+02, 1.8241e+02, 1.9358e+02],\n",
            "        [1.0809e+02, 7.1011e+01, 1.5887e+02, 1.2121e+02],\n",
            "        [9.6634e+01, 3.5168e+01, 1.4767e+02, 8.6349e+01],\n",
            "        [3.2207e+01, 2.6284e+02, 8.0077e+01, 3.0000e+02],\n",
            "        [1.5516e+02, 2.1525e+02, 2.0628e+02, 2.6693e+02],\n",
            "        [2.1695e+02, 7.1203e+01, 2.6784e+02, 1.2088e+02],\n",
            "        [1.9236e+02, 7.1281e+01, 2.4239e+02, 1.2112e+02],\n",
            "        [2.5176e+02, 8.2852e+01, 3.0276e+02, 1.3430e+02],\n",
            "        [0.0000e+00, 2.5602e+02, 1.5934e+02, 3.0000e+02],\n",
            "        [4.8639e+01, 1.3080e+02, 9.9873e+01, 1.8140e+02],\n",
            "        [1.8172e+01, 0.0000e+00, 3.1135e+01, 5.5670e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 1.6219e+01, 1.1602e+01],\n",
            "        [3.3744e+02, 8.1650e+01, 3.8872e+02, 1.3359e+02],\n",
            "        [1.0204e+01, 0.0000e+00, 4.0031e+01, 1.1983e+01],\n",
            "        [6.1124e+01, 2.4985e+02, 1.1099e+02, 3.0000e+02],\n",
            "        [3.6811e-01, 2.1376e+02, 5.0053e+01, 2.6523e+02],\n",
            "        [1.5476e+02, 2.6341e+02, 2.0397e+02, 3.0000e+02],\n",
            "        [0.0000e+00, 0.0000e+00, 1.0480e+02, 9.9388e+01],\n",
            "        [9.7607e+01, 2.3855e+02, 1.4732e+02, 2.8975e+02],\n",
            "        [3.3830e+02, 1.5356e+02, 3.8916e+02, 2.0477e+02],\n",
            "        [6.1211e+01, 1.6722e+02, 1.1115e+02, 2.1753e+02],\n",
            "        [4.8191e+01, 1.7897e+02, 9.8616e+01, 2.3096e+02],\n",
            "        [3.3969e+02, 2.3704e+02, 3.8966e+02, 2.8864e+02],\n",
            "        [8.3775e+01, 4.6235e+01, 1.3562e+02, 9.7021e+01],\n",
            "        [5.4079e-03, 0.0000e+00, 9.4027e+00, 5.1326e+00],\n",
            "        [2.6505e+02, 2.0329e+02, 3.1499e+02, 2.5410e+02],\n",
            "        [2.8905e+02, 1.5484e+02, 3.3928e+02, 2.0449e+02],\n",
            "        [8.5257e+01, 1.9122e+02, 1.3581e+02, 2.4149e+02],\n",
            "        [1.3273e+02, 9.5896e+01, 1.8250e+02, 1.4620e+02],\n",
            "        [1.5599e+02, 5.9575e+01, 2.0609e+02, 1.1049e+02],\n",
            "        [2.5604e+02, 2.6105e+02, 4.0000e+02, 3.0000e+02],\n",
            "        [0.0000e+00, 0.0000e+00, 8.7624e+01, 5.8907e+01],\n",
            "        [2.0403e+02, 1.0687e+02, 2.5529e+02, 1.5742e+02],\n",
            "        [3.3850e+02, 2.1486e+02, 3.8934e+02, 2.6476e+02],\n",
            "        [1.4349e+02, 4.6393e+01, 1.9412e+02, 9.7157e+01],\n",
            "        [1.9139e+02, 1.5412e+02, 2.4239e+02, 2.0555e+02],\n",
            "        [2.4003e+02, 1.1941e+02, 2.9042e+02, 1.6985e+02],\n",
            "        [3.1076e+02, 1.7887e+02, 3.6317e+02, 2.2920e+02],\n",
            "        [2.5304e+02, 3.5085e+01, 3.0493e+02, 8.5037e+01],\n",
            "        [2.5548e+01, 1.4261e+02, 7.6357e+01, 1.9294e+02],\n",
            "        [1.8127e+02, 1.7948e+02, 2.3087e+02, 2.2836e+02],\n",
            "        [9.4729e+01, 1.0625e+02, 1.4665e+02, 1.5657e+02],\n",
            "        [2.4440e+01, 3.4529e+01, 7.4103e+01, 8.5294e+01],\n",
            "        [1.3397e+01, 1.7828e+02, 6.4319e+01, 2.3015e+02],\n",
            "        [5.9576e+01, 2.2695e+02, 1.1057e+02, 2.7731e+02],\n",
            "        [0.0000e+00, 2.5262e+02, 4.1038e+01, 3.0000e+02],\n",
            "        [2.1637e+02, 1.5508e+02, 2.6629e+02, 2.0537e+02]],\n",
            "       grad_fn=<StackBackward0>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1]), 'scores': tensor([0.5217, 0.5216, 0.5163, 0.5148, 0.5062, 0.5056, 0.5054, 0.5054, 0.5052,\n",
            "        0.5050, 0.5049, 0.5048, 0.5047, 0.5044, 0.5043, 0.5043, 0.5043, 0.5043,\n",
            "        0.5042, 0.5042, 0.5042, 0.5041, 0.5041, 0.5040, 0.5039, 0.5038, 0.5038,\n",
            "        0.5037, 0.5037, 0.5036, 0.5036, 0.5036, 0.5036, 0.5035, 0.5035, 0.5034,\n",
            "        0.5033, 0.5032, 0.5032, 0.5032, 0.5032, 0.5032, 0.5032, 0.5031, 0.5031,\n",
            "        0.5031, 0.5031, 0.5031, 0.5031, 0.5031, 0.5030, 0.5030, 0.5029, 0.5029,\n",
            "        0.5029, 0.5029, 0.5029, 0.5029, 0.5028, 0.5028, 0.5028, 0.5028, 0.5027,\n",
            "        0.5027, 0.5026, 0.5026, 0.5026, 0.5026, 0.5026, 0.5026, 0.5025, 0.5025,\n",
            "        0.5025, 0.5025, 0.5025, 0.5025, 0.5024, 0.5024, 0.5023, 0.5023, 0.5023,\n",
            "        0.5023, 0.5023, 0.5022, 0.5022, 0.5022, 0.5022, 0.5022, 0.5022, 0.5022,\n",
            "        0.5022, 0.5021, 0.5021, 0.5021, 0.5020, 0.5020, 0.5020, 0.5020, 0.5020,\n",
            "        0.5020], grad_fn=<IndexBackward0>)}\n"
          ]
        }
      ],
      "source": [
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")\n",
        "\n",
        "from torchvision.transforms import v2 as T\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
        "    transforms.append(T.ToPureTensor())\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "\n",
        "\n",
        "# Testing ``forward()`` method (Optional)\n",
        "# ---------------------------------------\n",
        "#\n",
        "# 데이터셋을 반복하기 전에, 샘플 데이터로 학습과 추론 시 모델이 예상대로 동작하는지 살펴보는 것이 좋습니다.\n",
        "import utils\n",
        "\n",
        "dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "# For Training\n",
        "images, targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "output = model(images, targets)  # Returns losses and detections\n",
        "print(output)\n",
        "\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)  # Returns predictions\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJwJJQ_vNvoZ"
      },
      "source": [
        "### 모델 학습\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guY0qG7VNvoZ"
      },
      "outputs": [],
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "\n",
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and person\n",
        "num_classes = 2\n",
        "# use our dataset and defined transformations\n",
        "dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
        "dataset_test = PennFudanDataset('data/PennFudanPed', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(data_loader_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzCQFwTHjEFZ",
        "outputId": "2141638c-1276-449b-8982-4b67b3c75318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tensor([[[0.1176, 0.1176, 0.1255,  ..., 0.1765, 0.2627, 0.1961],\n",
              "           [0.1490, 0.1176, 0.1294,  ..., 0.1961, 0.6784, 0.7451],\n",
              "           [0.1451, 0.1294, 0.1686,  ..., 0.7216, 0.6392, 0.5686],\n",
              "           ...,\n",
              "           [0.4824, 0.4863, 0.4902,  ..., 0.5725, 0.5647, 0.5922],\n",
              "           [0.4824, 0.4902, 0.4941,  ..., 0.5725, 0.5647, 0.5647],\n",
              "           [0.4863, 0.4902, 0.4980,  ..., 0.5569, 0.5608, 0.5373]],\n",
              "  \n",
              "          [[0.1725, 0.1725, 0.1804,  ..., 0.1882, 0.2667, 0.2000],\n",
              "           [0.2039, 0.1725, 0.1843,  ..., 0.2078, 0.6824, 0.7490],\n",
              "           [0.2118, 0.1961, 0.2353,  ..., 0.7333, 0.6431, 0.5725],\n",
              "           ...,\n",
              "           [0.4824, 0.4863, 0.4902,  ..., 0.5725, 0.5647, 0.5922],\n",
              "           [0.4824, 0.4902, 0.4941,  ..., 0.5725, 0.5647, 0.5647],\n",
              "           [0.4863, 0.4902, 0.4980,  ..., 0.5569, 0.5608, 0.5373]],\n",
              "  \n",
              "          [[0.1137, 0.1137, 0.1216,  ..., 0.1529, 0.2353, 0.1686],\n",
              "           [0.1451, 0.1137, 0.1255,  ..., 0.1725, 0.6510, 0.7176],\n",
              "           [0.1412, 0.1255, 0.1647,  ..., 0.6980, 0.6196, 0.5490],\n",
              "           ...,\n",
              "           [0.4745, 0.4784, 0.4824,  ..., 0.5647, 0.5569, 0.5843],\n",
              "           [0.4745, 0.4824, 0.4863,  ..., 0.5647, 0.5569, 0.5569],\n",
              "           [0.4784, 0.4824, 0.4902,  ..., 0.5490, 0.5529, 0.5294]]]),),\n",
              " ({'boxes': tensor([[ 18.,  42., 173., 326.]]),\n",
              "   'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
              "            [0, 0, 0,  ..., 0, 0, 0],\n",
              "            [0, 0, 0,  ..., 0, 0, 0],\n",
              "            ...,\n",
              "            [0, 0, 0,  ..., 0, 0, 0],\n",
              "            [0, 0, 0,  ..., 0, 0, 0],\n",
              "            [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
              "   'labels': tensor([1]),\n",
              "   'image_id': 14,\n",
              "   'area': tensor([44020.]),\n",
              "   'iscrowd': tensor([0])},))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the model using our helper function\n",
        "# 모바일넷은 성능이 낮아 resnet 기반 모델로 다시 변경합니다.\n",
        "model = get_model_instance_segmentation(num_classes)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params,\n",
        "    lr=0.005,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=3,\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "# let's train it for 5 epochs\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "print(\"That's it!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZGIlZGijBZP",
        "outputId": "19439c9d-8481-434a-af51-56eeed0e7746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
            "100%|██████████| 170M/170M [00:01<00:00, 149MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0]  [ 0/60]  eta: 0:09:21  lr: 0.000090  loss: 4.3019 (4.3019)  loss_classifier: 0.7415 (0.7415)  loss_box_reg: 0.5109 (0.5109)  loss_mask: 2.9980 (2.9980)  loss_objectness: 0.0408 (0.0408)  loss_rpn_box_reg: 0.0107 (0.0107)  time: 9.3636  data: 0.5065  max mem: 2150\n",
            "Epoch: [0]  [10/60]  eta: 0:01:07  lr: 0.000936  loss: 1.6386 (2.2515)  loss_classifier: 0.4847 (0.4875)  loss_box_reg: 0.2661 (0.3066)  loss_mask: 0.7777 (1.4244)  loss_objectness: 0.0235 (0.0270)  loss_rpn_box_reg: 0.0039 (0.0060)  time: 1.3466  data: 0.0524  max mem: 2655\n",
            "Epoch: [0]  [20/60]  eta: 0:00:39  lr: 0.001783  loss: 1.1393 (1.5612)  loss_classifier: 0.2478 (0.3437)  loss_box_reg: 0.2642 (0.3027)  loss_mask: 0.4050 (0.8867)  loss_objectness: 0.0207 (0.0219)  loss_rpn_box_reg: 0.0046 (0.0062)  time: 0.5600  data: 0.0081  max mem: 2764\n",
            "Epoch: [0]  [30/60]  eta: 0:00:25  lr: 0.002629  loss: 0.6338 (1.2369)  loss_classifier: 0.1090 (0.2594)  loss_box_reg: 0.2581 (0.2856)  loss_mask: 0.2261 (0.6681)  loss_objectness: 0.0049 (0.0168)  loss_rpn_box_reg: 0.0070 (0.0071)  time: 0.5761  data: 0.0083  max mem: 2764\n",
            "Epoch: [0]  [40/60]  eta: 0:00:15  lr: 0.003476  loss: 0.4836 (1.0509)  loss_classifier: 0.0620 (0.2119)  loss_box_reg: 0.1926 (0.2661)  loss_mask: 0.1843 (0.5520)  loss_objectness: 0.0031 (0.0134)  loss_rpn_box_reg: 0.0070 (0.0074)  time: 0.5695  data: 0.0085  max mem: 2764\n",
            "Epoch: [0]  [50/60]  eta: 0:00:07  lr: 0.004323  loss: 0.3906 (0.9146)  loss_classifier: 0.0480 (0.1787)  loss_box_reg: 0.1580 (0.2416)  loss_mask: 0.1584 (0.4764)  loss_objectness: 0.0012 (0.0111)  loss_rpn_box_reg: 0.0051 (0.0069)  time: 0.5680  data: 0.0087  max mem: 2888\n",
            "Epoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2989 (0.8271)  loss_classifier: 0.0396 (0.1593)  loss_box_reg: 0.1011 (0.2204)  loss_mask: 0.1639 (0.4306)  loss_objectness: 0.0017 (0.0101)  loss_rpn_box_reg: 0.0034 (0.0068)  time: 0.5645  data: 0.0077  max mem: 2888\n",
            "Epoch: [0] Total time: 0:00:43 (0.7174 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/50]  eta: 0:00:35  model_time: 0.2351 (0.2351)  evaluator_time: 0.0140 (0.0140)  time: 0.7026  data: 0.4521  max mem: 2888\n",
            "Test:  [49/50]  eta: 0:00:00  model_time: 0.1098 (0.1205)  evaluator_time: 0.0044 (0.0081)  time: 0.1258  data: 0.0041  max mem: 2888\n",
            "Test: Total time: 0:00:07 (0.1503 s / it)\n",
            "Averaged stats: model_time: 0.1098 (0.1205)  evaluator_time: 0.0044 (0.0081)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.713\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.993\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.929\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.596\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.718\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.342\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.761\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.761\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.761\n",
            "IoU metric: segm\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.729\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.993\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.945\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.557\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.736\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.336\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.770\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.771\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.750\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.773\n",
            "Epoch: [1]  [ 0/60]  eta: 0:01:16  lr: 0.005000  loss: 0.4476 (0.4476)  loss_classifier: 0.0608 (0.0608)  loss_box_reg: 0.1585 (0.1585)  loss_mask: 0.2126 (0.2126)  loss_objectness: 0.0034 (0.0034)  loss_rpn_box_reg: 0.0123 (0.0123)  time: 1.2705  data: 0.6379  max mem: 2888\n",
            "Epoch: [1]  [10/60]  eta: 0:00:34  lr: 0.005000  loss: 0.2561 (0.2882)  loss_classifier: 0.0457 (0.0419)  loss_box_reg: 0.0796 (0.0949)  loss_mask: 0.1346 (0.1418)  loss_objectness: 0.0021 (0.0023)  loss_rpn_box_reg: 0.0053 (0.0074)  time: 0.6830  data: 0.0673  max mem: 3037\n",
            "Epoch: [1]  [20/60]  eta: 0:00:25  lr: 0.005000  loss: 0.2561 (0.2859)  loss_classifier: 0.0387 (0.0411)  loss_box_reg: 0.0796 (0.0938)  loss_mask: 0.1230 (0.1427)  loss_objectness: 0.0019 (0.0021)  loss_rpn_box_reg: 0.0042 (0.0062)  time: 0.6171  data: 0.0089  max mem: 3037\n",
            "Epoch: [1]  [30/60]  eta: 0:00:19  lr: 0.005000  loss: 0.2561 (0.2885)  loss_classifier: 0.0376 (0.0403)  loss_box_reg: 0.0902 (0.0972)  loss_mask: 0.1267 (0.1427)  loss_objectness: 0.0006 (0.0020)  loss_rpn_box_reg: 0.0042 (0.0062)  time: 0.6311  data: 0.0103  max mem: 3037\n",
            "Epoch: [1]  [40/60]  eta: 0:00:12  lr: 0.005000  loss: 0.2489 (0.2824)  loss_classifier: 0.0304 (0.0391)  loss_box_reg: 0.0685 (0.0895)  loss_mask: 0.1432 (0.1463)  loss_objectness: 0.0006 (0.0018)  loss_rpn_box_reg: 0.0042 (0.0058)  time: 0.6033  data: 0.0106  max mem: 3037\n",
            "Epoch: [1]  [50/60]  eta: 0:00:06  lr: 0.005000  loss: 0.2499 (0.2747)  loss_classifier: 0.0285 (0.0387)  loss_box_reg: 0.0559 (0.0848)  loss_mask: 0.1507 (0.1440)  loss_objectness: 0.0005 (0.0017)  loss_rpn_box_reg: 0.0042 (0.0055)  time: 0.5696  data: 0.0088  max mem: 3037\n",
            "Epoch: [1]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2620 (0.2750)  loss_classifier: 0.0348 (0.0378)  loss_box_reg: 0.0631 (0.0834)  loss_mask: 0.1507 (0.1464)  loss_objectness: 0.0016 (0.0019)  loss_rpn_box_reg: 0.0044 (0.0055)  time: 0.5868  data: 0.0092  max mem: 3037\n",
            "Epoch: [1] Total time: 0:00:37 (0.6244 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/50]  eta: 0:00:57  model_time: 0.1915 (0.1915)  evaluator_time: 0.0058 (0.0058)  time: 1.1575  data: 0.9590  max mem: 3037\n",
            "Test:  [49/50]  eta: 0:00:00  model_time: 0.1039 (0.1139)  evaluator_time: 0.0046 (0.0069)  time: 0.1243  data: 0.0071  max mem: 3037\n",
            "Test: Total time: 0:00:07 (0.1568 s / it)\n",
            "Averaged stats: model_time: 0.1039 (0.1139)  evaluator_time: 0.0046 (0.0069)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.730\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.992\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.940\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.734\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.732\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.343\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.775\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.775\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.838\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.770\n",
            "IoU metric: segm\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.731\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.992\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.943\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.465\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.738\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.344\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.777\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.777\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.778\n",
            "Epoch: [2]  [ 0/60]  eta: 0:01:03  lr: 0.005000  loss: 0.1831 (0.1831)  loss_classifier: 0.0130 (0.0130)  loss_box_reg: 0.0295 (0.0295)  loss_mask: 0.1399 (0.1399)  loss_objectness: 0.0000 (0.0000)  loss_rpn_box_reg: 0.0006 (0.0006)  time: 1.0606  data: 0.5251  max mem: 3037\n",
            "Epoch: [2]  [10/60]  eta: 0:00:31  lr: 0.005000  loss: 0.2026 (0.2287)  loss_classifier: 0.0245 (0.0298)  loss_box_reg: 0.0528 (0.0602)  loss_mask: 0.1268 (0.1329)  loss_objectness: 0.0008 (0.0010)  loss_rpn_box_reg: 0.0052 (0.0048)  time: 0.6274  data: 0.0548  max mem: 3037\n",
            "Epoch: [2]  [20/60]  eta: 0:00:24  lr: 0.005000  loss: 0.2078 (0.2240)  loss_classifier: 0.0286 (0.0295)  loss_box_reg: 0.0528 (0.0575)  loss_mask: 0.1241 (0.1315)  loss_objectness: 0.0006 (0.0013)  loss_rpn_box_reg: 0.0032 (0.0042)  time: 0.5903  data: 0.0104  max mem: 3037\n",
            "Epoch: [2]  [30/60]  eta: 0:00:18  lr: 0.005000  loss: 0.2110 (0.2302)  loss_classifier: 0.0297 (0.0325)  loss_box_reg: 0.0551 (0.0624)  loss_mask: 0.1190 (0.1299)  loss_objectness: 0.0007 (0.0012)  loss_rpn_box_reg: 0.0032 (0.0042)  time: 0.5955  data: 0.0111  max mem: 3037\n",
            "Epoch: [2]  [40/60]  eta: 0:00:12  lr: 0.005000  loss: 0.2204 (0.2356)  loss_classifier: 0.0308 (0.0331)  loss_box_reg: 0.0629 (0.0660)  loss_mask: 0.1213 (0.1311)  loss_objectness: 0.0007 (0.0011)  loss_rpn_box_reg: 0.0047 (0.0043)  time: 0.6024  data: 0.0089  max mem: 3037\n",
            "Epoch: [2]  [50/60]  eta: 0:00:06  lr: 0.005000  loss: 0.2537 (0.2454)  loss_classifier: 0.0301 (0.0341)  loss_box_reg: 0.0685 (0.0705)  loss_mask: 0.1279 (0.1347)  loss_objectness: 0.0005 (0.0012)  loss_rpn_box_reg: 0.0053 (0.0050)  time: 0.6159  data: 0.0087  max mem: 3037\n",
            "Epoch: [2]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2109 (0.2383)  loss_classifier: 0.0262 (0.0326)  loss_box_reg: 0.0561 (0.0672)  loss_mask: 0.1250 (0.1324)  loss_objectness: 0.0006 (0.0013)  loss_rpn_box_reg: 0.0033 (0.0048)  time: 0.5825  data: 0.0081  max mem: 3037\n",
            "Epoch: [2] Total time: 0:00:36 (0.6065 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/50]  eta: 0:00:47  model_time: 0.2170 (0.2170)  evaluator_time: 0.0079 (0.0079)  time: 0.9439  data: 0.7176  max mem: 3037\n",
            "Test:  [49/50]  eta: 0:00:00  model_time: 0.1009 (0.1118)  evaluator_time: 0.0030 (0.0046)  time: 0.1171  data: 0.0041  max mem: 3037\n",
            "Test: Total time: 0:00:07 (0.1440 s / it)\n",
            "Averaged stats: model_time: 0.1009 (0.1118)  evaluator_time: 0.0030 (0.0046)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.779\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.995\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.954\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.731\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.781\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.358\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.828\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.828\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.838\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.827\n",
            "IoU metric: segm\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.736\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.985\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.924\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.627\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.741\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.342\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.784\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.784\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.775\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.785\n",
            "Epoch: [3]  [ 0/60]  eta: 0:01:11  lr: 0.000500  loss: 0.1748 (0.1748)  loss_classifier: 0.0225 (0.0225)  loss_box_reg: 0.0408 (0.0408)  loss_mask: 0.1089 (0.1089)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0026 (0.0026)  time: 1.1942  data: 0.5656  max mem: 3037\n",
            "Epoch: [3]  [10/60]  eta: 0:00:32  lr: 0.000500  loss: 0.1893 (0.2046)  loss_classifier: 0.0225 (0.0279)  loss_box_reg: 0.0408 (0.0518)  loss_mask: 0.1089 (0.1213)  loss_objectness: 0.0002 (0.0005)  loss_rpn_box_reg: 0.0026 (0.0031)  time: 0.6517  data: 0.0625  max mem: 3037\n",
            "Epoch: [3]  [20/60]  eta: 0:00:24  lr: 0.000500  loss: 0.1986 (0.2017)  loss_classifier: 0.0225 (0.0277)  loss_box_reg: 0.0438 (0.0499)  loss_mask: 0.1084 (0.1207)  loss_objectness: 0.0002 (0.0006)  loss_rpn_box_reg: 0.0026 (0.0030)  time: 0.5888  data: 0.0109  max mem: 3037\n",
            "Epoch: [3]  [30/60]  eta: 0:00:18  lr: 0.000500  loss: 0.2048 (0.2140)  loss_classifier: 0.0260 (0.0301)  loss_box_reg: 0.0442 (0.0548)  loss_mask: 0.1245 (0.1249)  loss_objectness: 0.0003 (0.0007)  loss_rpn_box_reg: 0.0028 (0.0035)  time: 0.5878  data: 0.0099  max mem: 3037\n",
            "Epoch: [3]  [40/60]  eta: 0:00:11  lr: 0.000500  loss: 0.1808 (0.2047)  loss_classifier: 0.0235 (0.0286)  loss_box_reg: 0.0376 (0.0501)  loss_mask: 0.1140 (0.1219)  loss_objectness: 0.0004 (0.0007)  loss_rpn_box_reg: 0.0037 (0.0033)  time: 0.5770  data: 0.0112  max mem: 3037\n",
            "Epoch: [3]  [50/60]  eta: 0:00:06  lr: 0.000500  loss: 0.1808 (0.2081)  loss_classifier: 0.0234 (0.0287)  loss_box_reg: 0.0414 (0.0521)  loss_mask: 0.1093 (0.1228)  loss_objectness: 0.0005 (0.0008)  loss_rpn_box_reg: 0.0037 (0.0037)  time: 0.5840  data: 0.0102  max mem: 3037\n",
            "Epoch: [3]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.1766 (0.2017)  loss_classifier: 0.0277 (0.0282)  loss_box_reg: 0.0369 (0.0487)  loss_mask: 0.1093 (0.1205)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0027 (0.0035)  time: 0.5973  data: 0.0081  max mem: 3039\n",
            "Epoch: [3] Total time: 0:00:36 (0.6051 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/50]  eta: 0:00:34  model_time: 0.1622 (0.1622)  evaluator_time: 0.0054 (0.0054)  time: 0.6951  data: 0.5261  max mem: 3039\n",
            "Test:  [49/50]  eta: 0:00:00  model_time: 0.1004 (0.1114)  evaluator_time: 0.0033 (0.0047)  time: 0.1171  data: 0.0036  max mem: 3039\n",
            "Test: Total time: 0:00:06 (0.1395 s / it)\n",
            "Averaged stats: model_time: 0.1004 (0.1114)  evaluator_time: 0.0033 (0.0047)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.818\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.996\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.959\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.755\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.822\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.380\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.864\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.864\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.850\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.865\n",
            "IoU metric: segm\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.753\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.986\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.946\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.672\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.756\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.797\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.797\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.799\n",
            "Epoch: [4]  [ 0/60]  eta: 0:01:21  lr: 0.000500  loss: 0.1206 (0.1206)  loss_classifier: 0.0105 (0.0105)  loss_box_reg: 0.0143 (0.0143)  loss_mask: 0.0945 (0.0945)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0010 (0.0010)  time: 1.3510  data: 0.6362  max mem: 3039\n",
            "Epoch: [4]  [10/60]  eta: 0:00:33  lr: 0.000500  loss: 0.1761 (0.2019)  loss_classifier: 0.0223 (0.0271)  loss_box_reg: 0.0366 (0.0481)  loss_mask: 0.1146 (0.1227)  loss_objectness: 0.0004 (0.0005)  loss_rpn_box_reg: 0.0031 (0.0036)  time: 0.6716  data: 0.0674  max mem: 3039\n",
            "Epoch: [4]  [20/60]  eta: 0:00:24  lr: 0.000500  loss: 0.1698 (0.1857)  loss_classifier: 0.0204 (0.0245)  loss_box_reg: 0.0324 (0.0407)  loss_mask: 0.1114 (0.1168)  loss_objectness: 0.0003 (0.0005)  loss_rpn_box_reg: 0.0027 (0.0032)  time: 0.5779  data: 0.0092  max mem: 3039\n",
            "Epoch: [4]  [30/60]  eta: 0:00:18  lr: 0.000500  loss: 0.1597 (0.1799)  loss_classifier: 0.0203 (0.0247)  loss_box_reg: 0.0273 (0.0384)  loss_mask: 0.1022 (0.1131)  loss_objectness: 0.0003 (0.0009)  loss_rpn_box_reg: 0.0021 (0.0028)  time: 0.5638  data: 0.0102  max mem: 3039\n",
            "Epoch: [4]  [40/60]  eta: 0:00:12  lr: 0.000500  loss: 0.1740 (0.1908)  loss_classifier: 0.0268 (0.0259)  loss_box_reg: 0.0370 (0.0426)  loss_mask: 0.1117 (0.1182)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0029 (0.0031)  time: 0.5867  data: 0.0108  max mem: 3039\n",
            "Epoch: [4]  [50/60]  eta: 0:00:05  lr: 0.000500  loss: 0.2049 (0.1933)  loss_classifier: 0.0286 (0.0264)  loss_box_reg: 0.0448 (0.0434)  loss_mask: 0.1151 (0.1192)  loss_objectness: 0.0004 (0.0011)  loss_rpn_box_reg: 0.0037 (0.0032)  time: 0.5935  data: 0.0090  max mem: 3039\n",
            "Epoch: [4]  [59/60]  eta: 0:00:00  lr: 0.000500  loss: 0.2032 (0.1932)  loss_classifier: 0.0298 (0.0265)  loss_box_reg: 0.0408 (0.0433)  loss_mask: 0.1196 (0.1192)  loss_objectness: 0.0006 (0.0011)  loss_rpn_box_reg: 0.0027 (0.0031)  time: 0.6050  data: 0.0090  max mem: 3039\n",
            "Epoch: [4] Total time: 0:00:36 (0.6101 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/50]  eta: 0:00:34  model_time: 0.2063 (0.2063)  evaluator_time: 0.0055 (0.0055)  time: 0.6840  data: 0.4708  max mem: 3039\n",
            "Test:  [49/50]  eta: 0:00:00  model_time: 0.1112 (0.1171)  evaluator_time: 0.0043 (0.0060)  time: 0.1320  data: 0.0087  max mem: 3039\n",
            "Test: Total time: 0:00:07 (0.1491 s / it)\n",
            "Averaged stats: model_time: 0.1112 (0.1171)  evaluator_time: 0.0043 (0.0060)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.830\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.995\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.949\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.757\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.834\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.870\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.870\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.850\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.871\n",
            "IoU metric: segm\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.751\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.985\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.943\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.593\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.757\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.346\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.797\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.797\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.762\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.800\n",
            "That's it!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbImbVPhNvoZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "d6a2fec8-b99a-424e-cc85-d0ed4e5b86c8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a51737d1b403>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_bounding_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_segmentation_masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wget https://tutorials.pytorch.kr/_static/img/tv_tutorial/tv_image05.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malexnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mefficientnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgooglenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/convnext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2dNormActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStochasticDepth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_presets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_register_onnx_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_register_custom_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from .boxes import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatched_nms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbox_area\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbox_convert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/ops/_register_onnx_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msymbolic_opset11\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopset11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from . import (  # usort:skip. Keep the order instead of sorting lexicographically\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0m_deprecation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/errors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdiagnostics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m __all__ = [\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/diagnostics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from ._diagnostic import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcreate_export_diagnostic_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdiagnose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mexport_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/diagnostics/_diagnostic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnostics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnostics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msarif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnostics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msarif\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msarif_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/diagnostics/infra/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from ._infra import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mDiagnosticOptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mGraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mInvocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mLevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/diagnostics/infra/_infra.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFrozenSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnostics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msarif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/diagnostics/infra/formatter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_beartype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnostics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msarif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/diagnostics/infra/sarif/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mConfigurationOverride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnostics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msarif\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnostics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msarif\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEdge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnostics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msarif\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edge_traversal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEdgeTraversal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/diagnostics/infra/sarif/_conversion.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m from torch.onnx._internal.diagnostics.infra.sarif import (\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0m_artifact_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0m_invocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_verbose_message\u001b[0;34m(message, verbosity, *args)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
        "\n",
        "os.system(\"wget https://tutorials.pytorch.kr/_static/img/tv_tutorial/tv_image05.png\")\n",
        "\n",
        "image = read_image(\"/content/tv_image05.png\")\n",
        "eval_transform = get_transform(train=False)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x = eval_transform(image)\n",
        "    # convert RGBA -> RGB and move to device\n",
        "    x = x[:3, ...].to(device)\n",
        "    predictions = model([x, ])\n",
        "    pred = predictions[0]\n",
        "\n",
        "\n",
        "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
        "image = image[:3, ...]\n",
        "pred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
        "pred_boxes = pred[\"boxes\"].long()\n",
        "output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n",
        "\n",
        "masks = (pred[\"masks\"] > 0.7).squeeze(1)\n",
        "output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(output_image.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VwsnZ7ZBck2d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}